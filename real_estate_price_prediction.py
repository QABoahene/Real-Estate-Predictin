# -*- coding: utf-8 -*-
"""Real Estate Price Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c7LzcfjVXPd6CZ1kU9ceAYxioJoDhi9L

This is to predict the real estate prices or bids for houses in Bengaluru, India.

This is my commented version following a playlist of tutorial for this project. Find the tutorial here: https://www.youtube.com/playlist?list=PLeo1K3hjS3uu7clOTtwsp94PcHbzqpAdg
Dataset: https://www.kaggle.com/amitabhajoy/bengaluru-house-price-data
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing Libraries
import pandas as pd
import io
import numpy as np
import pickle
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
# %matplotlib inline
import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)

#This is to upload files from my local disk
from google.colab import files
uploaded = files.upload()

real_estate = pd.read_csv(io.BytesIO(uploaded['Bengaluru_House_Data.csv']))
# Dataset is now stored in a Pandas Dataframe

real_estate.head(5)

real_estate.shape #This shows the number of columns and the number of rows in the dataset.

real_estate.columns #I am now showing the column names of the dataset as this will help us navigate through it during feature extraction.

'''
This is to examine the feature 'area_type'.
That feature is in categories so the code below groups a list 'area_type' 
by it's categories then aggregrate to give the count of each categories.
'''
real_estate.groupby('area_type')['area_type'].agg('count')

"""Now, it isn't all the columns above that is important to us. So columns from the dataset will be dropped in order to begin the cleaning process of the dataset. Note that you should be **very sure** that the columns you're dropping won't be important during feature engineering.

We assume that area_type, availability, society and balcony will not affect the price of the building. This is only assumed. You have to ask yourself or your employer whether a feature is important or not. Sometimes a column/feature may not be important but that could help you in feature engineering to create new features. This also depends on how well you know your data so take time and go through the columns of a dataset, what it was collected for and what you want to achieve with it. That will help you decide.
"""

clean_estate = real_estate.drop(['area_type','society','balcony','availability'], axis = 'columns')

'''So here, remember that if it was just a column that we were dropping, it will just be "....('column_name')
but the square brackets were used because we were providing a list of columns to be deleted simultaneously.
the 'axis = columns' shows that we want to delete the columns. 
It could be coded as 'axis = 1', this is the same as axis = columns.'''

clean_estate.head(5)

clean_estate.isnull().sum() #This code will be showing us the sum (.sum()) of all the empty values in the columns.

"""This shows us the degree of missing values we have in a column. With this, you'll be able to inform your decision to clean the data. With this kind of cleaning, you can decide to drop the rows if the amount of missing value is not too high. Also, you can decide to fill the missing values with the median values of the column. 
Remember that this has the potential of affecting the effectiveness of your dataset so you have to make this decision carefully.
"""

cl_estate = clean_estate.dropna() #This drops the rows that are null
cl_estate.isnull().sum()

#We can now have a look at the shape of the dataset after this part of the cleaning. 
cl_estate.shape

"""Now, looking at the size column, we can see that the data has different types of inputs. It is important that we explore it and make cleaning decisions about it."""

cl_estate['size'].unique() #This will give you a pandas series and then call the unique values of the column

"""Note that there are values with '4 BHK' and '4 Bedroom'. These are essentially the same and you have to always be on the look out for such problems in the dataset. some things may be the same but because of bad labelling, they might end up being differentiaed and this can affect the quality of your dataset.  A new column will be created to help deal with this problem."""

cl_estate['bhk'] = cl_estate['size'].apply(lambda x: int(x.split(' ')[0]))
cl_estate.head(5)

"""Now the above may seem a bit too much because I'm a beginner in both Python and Data Science but here is the best possible way I understood it. 
So with this, we created a new column called 'bhk' and this was based of the pre-existing 'size' column. since we stated above that same sizes were split into 2 because of their strings, we will be applying a function which will help us tokenize the strings and pick what we want exactly from the string. We will use the lambda function. Here's the thing, we will be tokenizing the string with space(' '). So we will be spliting the string input with spaces and then we will choose which one we will like. In this case we will be dealing with the numbers only since '4 Bedroom' is the same as '4 bhk'. The code above shows that the new column was created based off the size column and the lambda function was applied and then the spliting of the x values (which are the inputs) was done with spaces. Then [0] was chosen since that will be the first value we need from the spring. Now when dealing with the numbers which are tokenized as strings, we will be making them integers again to make the data appropriate to work with, hence the (....int(...)) part of the code.
"""

cl_estate['bhk'].unique() 
'''This now shows just the various numbers of bhk categoeries 
there are in the dataset and this looks cleaner and simpler than before'''

cl_estate[cl_estate.bhk>15]

"""Looking at the table above, you can see that some of the houses has a high bhk but the total square foot can be very questionable compared to that of the houses with high bhk. This will need some exploring. Always check your data presentation and try to find the next thing you can do to make your data better."""

cl_estate.total_sqft.unique #Again, this displays the unique values of the total_sqft column

"""With the variations, there are some ranges in the total_sqft column. This is not good for processing. A function is defined to detect if a given value in the column is float or not. 
The way the function works is that it will try to convert a given value (x) in the total_sqft column into float and if it isn't a valued value, it will be brought into the except where it will be returned as false or true.
"""

def is_float(x):
  try:
    float(x)
  except:
    return False
  return True

cl_estate[~cl_estate['total_sqft'].apply(is_float)].head(10)

"""There are many times that when cleaning a dataset, you come across things like we have in the total_sqft column. As you can see, there are ranges and inputs like '34.46Sq. Meter'. We have to be able to get constant input in order to make the dataset easier to deal with. 
So in this dataset, the way we will deal with the ranges will be that when we have a range, we take the average of the range. Rows with inputs like '34.46Sq. Meter' and '4125Perch' will be ignored. For a more robust and efficient data, you can do unit convesions for the other inputs mentioned above.
For now we will just deal with the ranges only.
"""

def convert_sqft(x): #Inputs string (x) is taken here
  tokens = x.split('-') #The string is split using ('-')
  if len(tokens) == 2: #So if the token is two
    return (float(tokens[0])+float(tokens[1]))/2 #Then the tokens are taken and the average is calculated for
  try:
    return float(x)
  except:
    return None
#If not, then the ssingle input will be converted to float

"""Trials"""

convert_sqft('2345') #The function will convert the simple string into a float.

convert_sqft('1015 - 1540') #The function will get the average of this and then return it as a float

#Confirmation
A = (1015 + 1540)/2
A

#Now we will apply the function to the 'total_sqft' column
cl_estate2 = cl_estate.copy() #We create a deep copy of the dataset 
cl_estate2['total_sqft'] = cl_estate2['total_sqft'].apply(convert_sqft)
cl_estate2.head(10)

#Now we check for a particular range input for verification of conversion
cl_estate2.loc[549]

#Confirmation
(1195 + 1440)/2

"""## Feature Engineering

Just as discussed above, feature engineering helps us to create features from the dataset which can help us make our dataset more efficient. So for this dataset, we created the price per square foot because this can help predict the prices accurately.
"""

cl_estate3 = cl_estate2.copy()
cl_estate3['price_per_sqft'] = cl_estate3['price']*100000/cl_estate3['total_sqft']
cl_estate3.head()

cl_estate3.location.unique() #This shows the unique locations in the column

len(cl_estate3.location.unique()) #This shows the number of locations in the column

"""The above is a high dimensionality issue because having that many locations will make it have a lot of columns and some of these locations may have one or two data points."""

#This is how I find the above
cl_estate3.location = cl_estate3.location.apply(lambda x: x.strip()) #This strips down any spaces in the location column
location_stats = cl_estate3.groupby('location')['location'].agg('count').sort_values(ascending = False) #The stats on the data points of the locations
location_stats

len(location_stats[location_stats<=10]) #This shows the number of locations with less than 10 data points in the dataset

location_stats_less_than_10 = location_stats[location_stats<=10]
location_stats_less_than_10 #We now create a new dataframe for the locations with less than 10 rows
#This will be later on put in a category called 'other'.

len(cl_estate3.location.unique()) #This is the number of locations in 'cl_estate3'.

cl_estate3.location = cl_estate3.location.apply(lambda x: 'other' if x in location_stats_less_than_10 else x)
len(cl_estate3.location.unique())
#A transformation is applied which will put a location in category 'other' if it has less than 10 data points
#The locations are now reduced to 242 locations which can be worked with because of the dimensionality reduction done.

cl_estate3.head(10)

"""## Outlier Detection and removal
Outliers are the data points which are errors, usually not all are errors but then they represent extreme variations in a dataset. They are valid but then they have the possibilities to create problems so removing them can be essential.
This can take some techniques and it also can depend on some domain expertise.
"""

#Rows in the dataset
cl_estate3.shape

cl_estate4 = cl_estate3[~(cl_estate3.total_sqft/cl_estate3.bhk<300)] #Negating the data rows which were filtered
cl_estate4.shape

cl_estate4.price_per_sqft.describe()

#Removing price per square foot outliers per location
def del_pps_outliers(cl_estate): #Taking input
  cl_estate_out = pd.DataFrame()
  for key, sub_cl_estate in cl_estate.groupby('location'): #Grouping by location
    m =  np.mean(sub_cl_estate.price_per_sqft) #Calculating mean per location
    st = np.std(sub_cl_estate.price_per_sqft) #Calculating standard deviation per location
    reduced_cl_estate = sub_cl_estate[(sub_cl_estate.price_per_sqft>(m-st)) & (sub_cl_estate.price_per_sqft<=(m+st))]
    #Filtering data points which are beyond the standard deviation
    cl_estate_out = pd.concat([cl_estate_out, reduced_cl_estate], ignore_index=True)
  return cl_estate_out

cl_estate5 = del_pps_outliers(cl_estate4)
 cl_estate5.shape

def scat_plot (cl_estate, location):
  bhk2 = cl_estate[(cl_estate.location==location) & (cl_estate.bhk==2)]
  bhk3 = cl_estate[(cl_estate.location==location) & (cl_estate.bhk==3)]
  #The above of the function two dataframes
  matplotlib.rcParams['figure.figsize'] = (15,10)
  plt.scatter(bhk2.total_sqft,bhk2.price,color = 'red', label = '2 BHK', s = 50)
  plt.scatter(bhk3.total_sqft,bhk3.price,marker='*', color = 'blue', label = '3 BHK', s = 50)
  #Creating a scatter plot
  plt.xlabel("TOTAL SQUARE FOOT AREA")
  plt.ylabel("PRICE")
  plt.title(location)
  plt.legend()

scat_plot(cl_estate5, "Rajaji Nagar")

scat_plot(cl_estate5, "Hebbal")

"""Per further analysis, we have to create a function that will also remove properties that are in the same location that has inconsistencies in pricing regarding to how it relates to the bedrooms.
For example, If the price of a three bedroom house is lesser than a two bedroom apartment but has the same sqare foot area. This is treated as an outlier because it is based on the locations and also domain expertise in real estate.

A dictionary is created for the stats per bhk for a specific location.

{
  '1' : {
    'mean': 4000,
    'std': 2000,
    'count': 34
  },
  '2' : {
    'mean': 4300,
    'std': 2300,
    'count': 22
  },
}

Now the 2 BHK houses which has price per sqare foot less than the mean price per square feet of 1BHK can be removed.
"""

#A function to delete BHK outliers
def del_bhk_outliers(cl_estate):
  exclude_indices = np.array([])
  for location, location_cl_estate in cl_estate.groupby('location'): #Grouping by location (every location dataframe)
    bhk_stats = {}
    for bhk, bhk_cl_estate in location_cl_estate.groupby('bhk'): #Creating new dataframes per bhk
      bhk_stats[bhk] = {
          'mean': np.mean(bhk_cl_estate.price_per_sqft),
          'std': np.std(bhk_cl_estate.price_per_sqft),
          'count': bhk_cl_estate.shape[0]
          #Calculating mean, standard deviation and count per bhk
      }
    for bhk, bhk_cl_estate in location_cl_estate.groupby('bhk'):
      stats = bhk_stats.get(bhk-1)
      if stats and stats['count'] > 5:
        exclude_indices = np.append(exclude_indices, bhk_cl_estate[bhk_cl_estate.price_per_sqft<(stats['mean'])].index.values)
  return cl_estate.drop(exclude_indices, axis = 'index')
  #Excluding data points with price per square feet less than the mean of the previous

cl_estate6 = del_bhk_outliers(cl_estate5)
cl_estate6.shape

#Plotting the same scatter plot to compare the outliers
scat_plot(cl_estate6, "Hebbal")

#Plotting a histogram to show how many properties there are per square feet area
matplotlib.rcParams["figure.figsize"] = (15,10)
plt.hist(cl_estate6.price_per_sqft, rwidth=0.8, color='red')
plt.xlabel("PRICE PER SQUARE FOOT")
plt.ylabel("COUNT")
plt.title("HISTOGRAM OF PROPERTIES PER PRICE PER SQUARE FOOT")

"""This distribution looks fine, normalised."""

#Exploring the bathroom feature
cl_estate6.bath.unique()

#Checking for the apartments with bathrooms more than ten(10)
cl_estate6[cl_estate6.bath>10]

"""Domain expertise will be able to tell you the things you need to know to deal with the bathroom feature.
For example, it wouldn't make sense to have 4 bathrooms in a house with one or two bedrooms. That's quite unusual. So this can be worked on, especially if the square foot area is less.
"""

#Exploring the bath feature with a histogram
plt.hist(cl_estate6.bath, rwidth=0.8)
plt.xlabel("NUMBER OF BATHROOMS")
plt.ylabel("COUNT")
plt.title("BATHROOM COUNT")

"""Now per the example above, we can say that any house with bathrooms greater (+2) than the number of bedrooms is or can be treated as an outlier."""

cl_estate6[cl_estate6.bath>cl_estate6.bhk+2]

cl_estate7 = cl_estate6[cl_estate6.bath<cl_estate6.bhk+2]
cl_estate7.shape

"""Now that the data is clean enough, it can be prepared for machine learning training. This means some features would have to be further dropped. Note that some features were kept to be able to inform how we treat other important features so always be informed about your features and how to drop them and when."""

#Creating a new dataframe without the dropped features for the machine learning training
final_data = cl_estate7.drop(['size', 'price_per_sqft'], axis='columns')
final_data.head()

"""# Model Building

Machine learning models can't interpret text data so the loctation columnn in the dataset would be transformed to numerical data. Since the location names are not categorized and are many, they will be transformed into dummy inputs. 
Pandas dummy method will be used.
"""

dummies = pd.get_dummies(final_data['location'])
dummies.head(5)
'''This creates dummy values for all the locations which
are turned into columns for this to take effect.'''

final_data2 = pd.concat([final_data, dummies.drop('other', axis = 1)], axis = 1)
final_data2.head(5)

final_data3 = final_data2.drop('location', axis = 1)
final_data3.head(5)

final_data3.shape

X = final_data3.drop('price', axis = 1) #Dropping the price because that is what we want to predict, leaving the depending variables
X.head()

Y = final_data3.price
Y.head()

#Dividing the dataset into training and test samples
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 10) # 20% will be for test sample

''' A linear regresion model is created,
a fit method is is called on X and Y train and 
a score is evaluated which will tell us how good our model is.'''

lr_clf = LinearRegression()
lr_clf.fit(X_train, Y_train)
lr_clf.score(X_test,Y_test)

"""The score is about 84% and as usual a couple of models with different parameters will be tried to give the best optimal model.

First we use K Fold cross validation
"""

Cross_Val = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) #Creating a Shuffle split which will randomize the sample for equal distribution
cross_val_score(LinearRegression(), X, Y, cv = Cross_Val)

"""Different regression algorithms would be used to try and find which will give the best score. For that a method called GridSearchCV will be used. This can run the model on different regressors and different parameters to let you know the best score."""

def gridsearchcv_best_model (X,Y):
  algos = { 
      'linear_regression' : {
          'model': LinearRegression(),
          'params': {
              'normalize': [True, False]
          }
      },
      'lasso': {
          'model': Lasso(),
          'params': {
              'alpha': [1,2],
              'selection': ['random', 'cyclic']
          }
      },
      'decision_tree': {
          'model': DecisionTreeRegressor(),
          'params': {
              'criterion': ['mse', 'friedman_mse'],
              'splitter': ['best', 'random']
          }
      }
  }
  scores = []
  Cross_Val = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
  for algo_name, config in algos.items():
    gs = GridSearchCV(config['model'], config['params'], cv = Cross_Val, return_train_score=False)
    gs.fit(X,Y)
    scores.append({
        'model': algo_name,
        'best_score': gs.best_score_,
        'best_params': gs.best_params_
    })

  return pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])

gridsearchcv_best_model(X,Y)

def price_prediction(location, sqft, bath, bhk):
  loc_index = np.where(X.columns==location)[0][0]

  x = np.zeros(len(X.columns))
  x[0] = sqft
  x[1] = bath
  x[2] = bhk
  if loc_index >= 0:
    x[loc_index] = 1

  return lr_clf.predict([x])[0]

price_prediction('1st Phase JP Nagar', 1000, 2, 2)

price_prediction('1st Phase JP Nagar', 1000, 3, 3)

price_prediction('Indira Nagar', 1000, 2, 2)

price_prediction('Indira Nagar', 1000, 3, 3)

#Exporting the model
with open ('bengaluru_home_price_model.pickle','wb') as f:
  pickle.dump(lr_clf, f)

import json
columns = {
    'data_columns': [col.lower() for col in X.columns]
}
with open('columns.json', 'w')as f:
  f.write(json.dumps(columns))

